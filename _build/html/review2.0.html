

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>review2 &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'review2.0';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="review3" href="review3.0.html" />
    <link rel="prev" title="review1" href="review1.0.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to MAST90138
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="basics.html">Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="useful_link.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="progress.html">Progress Check</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week1.0.html">week1</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week1.1.html">Week 1 Lecture 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.2.html">week1 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.3.html">week1 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week2.0.html">week2</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week2.1.html">Week 2 Lecture 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.2.html">Week 2 Lecture 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.3.html">week2 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week3.0.html">week3</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week3.1.html">Week 3 Lecture 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.2.html">week3 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.3.html">week3 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week4.0.html">week4</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week4.1.html">week4 lec 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.2.html">week4 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.3.html">week4 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week5.0.html">week5</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week5.1.html">week5 lec 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.2.html">week5 lec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.3.html">week5 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week6.0.html">week6</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week6.1.html">week6 lec1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.3.html">week 6 additional resources</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week7.0.html">week7</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week7.1.html">week7 lec1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.2.html">week7 lec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.3.html">week7 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week8.0.html">week8</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week8.1.html">week8 lec 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week8.2.html">week8 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week8.3.html">week8 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week9.0.html">week9</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week9.1.html">week9 lec 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week9.2.html">week9 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week9.3.html">week9 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week10.0.html">week10</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week10.1.html">week10 lec 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.2.html">week10 lec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.3.html">week10 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week11.0.html">week11</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week11.1.html">week11. lec1</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.2.html">week11. lec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.3.html">week11 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week12.0.html">week12</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week12.1.html">CLUSTERING ALGORITHMS</a></li>
<li class="toctree-l2"><a class="reference internal" href="week12.2.html">week12 lec 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week12.3.html">week12 additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="review0.html">Review</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="review1.0.html">review1</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">review2</a></li>
<li class="toctree-l2"><a class="reference internal" href="review3.0.html">review3</a></li>
<li class="toctree-l2"><a class="reference internal" href="review4.0.html">review4</a></li>
<li class="toctree-l2"><a class="reference internal" href="review5.0.html">review5</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Freview2.0.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/review2.0.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>review2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-cont">PCA Cont</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-decomposition">Eigenvalue decomposition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-matrix">Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decomposition-of-a-covariance-matrix">Decomposition of a Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-decompose-a-covariance-matrix">Why Decompose a Covariance Matrix?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-ols-changed">How OLS changed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-regression-pcr">Principal Component Regression (PCR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitation-of-pcr">Limitation of PCR</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-trade-off">Bias-Variance Trade-off</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-pcr">Limitations of PCR</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-is-traded-off-against-bias">Why Variance is Traded Off Against Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-pcr-and-pls">Difference Between PCR and PLS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Principal Component Regression (PCR):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-least-squares-pls-regression">Partial Least Squares (PLS) Regression:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases:</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="review2">
<h1>review2<a class="headerlink" href="#review2" title="Permalink to this heading">#</a></h1>
<section id="pca-cont">
<h2>PCA Cont<a class="headerlink" href="#pca-cont" title="Permalink to this heading">#</a></h2>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca1.jpg"><img alt="pca" class="bg-primary mb-1 align-center" src="_images/pca1.jpg" style="width: 800px;" /></a>
<ul class="simple">
<li><p>the reason we centralise data: so that calculating the variance of projected dots on pc is equivalent to calculating the sum of the distance of each dot to the center (0,0) =&gt; more interpretable and simplify calculation process.</p></li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca2.png"><img alt="pca" class="bg-primary mb-1 align-center" src="_images/pca2.png" style="width: 800px;" /></a>
<p>singular value decomposition (SVD): to “recover” the point we draw.</p>
</section>
<section id="eigenvalue-decomposition">
<h2>Eigenvalue decomposition<a class="headerlink" href="#eigenvalue-decomposition" title="Permalink to this heading">#</a></h2>
<p>Eigenvalue decomposition is a technique in linear algebra where a matrix is broken down into its constituent parts to make certain operations on the matrix easier to perform. This decomposition is especially useful in the context of covariance matrices when analyzing data.</p>
<p>Let’s go step by step to understand how the eigenvalue decomposition of a covariance matrix works and why it’s useful.</p>
<section id="covariance-matrix">
<h3>Covariance Matrix<a class="headerlink" href="#covariance-matrix" title="Permalink to this heading">#</a></h3>
<p>Firstly, the covariance matrix is a square matrix that summarizes the covariance (a measure of how much two variables change together) between each pair of elements in a data set. If you have a data set with <span class="math notranslate nohighlight">\( n \)</span> dimensions, the covariance matrix will be <span class="math notranslate nohighlight">\( n \times n \)</span>.</p>
</section>
<section id="eigenvalues-and-eigenvectors">
<h3>Eigenvalues and Eigenvectors<a class="headerlink" href="#eigenvalues-and-eigenvectors" title="Permalink to this heading">#</a></h3>
<p>An eigenvector of a square matrix <span class="math notranslate nohighlight">\( A \)</span> is a non-zero vector <span class="math notranslate nohighlight">\( v \)</span> such that when <span class="math notranslate nohighlight">\( A \)</span> is multiplied by <span class="math notranslate nohighlight">\( v \)</span>, the product is a scalar multiple of <span class="math notranslate nohighlight">\( v \)</span>. That scalar is known as an eigenvalue. Mathematically, this is represented as:</p>
<div class="math notranslate nohighlight">
\[
A v = \lambda v
\]</div>
<p>where <span class="math notranslate nohighlight">\( A \)</span> is the matrix, <span class="math notranslate nohighlight">\( v \)</span> is the eigenvector, and <span class="math notranslate nohighlight">\( \lambda \)</span> is the eigenvalue.</p>
</section>
<section id="decomposition-of-a-covariance-matrix">
<h3>Decomposition of a Covariance Matrix<a class="headerlink" href="#decomposition-of-a-covariance-matrix" title="Permalink to this heading">#</a></h3>
<p>When you decompose a covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span>, you find its eigenvalues and eigenvectors. This decomposition has the form:</p>
<div class="math notranslate nohighlight">
\[
\Sigma = Q \Lambda Q^{-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\( \Sigma \)</span> is the covariance matrix, <span class="math notranslate nohighlight">\( Q \)</span> is the matrix composed of the eigenvectors of <span class="math notranslate nohighlight">\( \Sigma \)</span>, <span class="math notranslate nohighlight">\( \Lambda \)</span> is the diagonal matrix with the eigenvalues of <span class="math notranslate nohighlight">\( \Sigma \)</span> on the diagonal, and <span class="math notranslate nohighlight">\( Q^{-1} \)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\( Q \)</span>.</p>
</section>
<section id="why-decompose-a-covariance-matrix">
<h3>Why Decompose a Covariance Matrix?<a class="headerlink" href="#why-decompose-a-covariance-matrix" title="Permalink to this heading">#</a></h3>
<p>Decomposing a covariance matrix is useful for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Principal Component Analysis (PCA):</strong> PCA is a technique to reduce the dimensionality of data. It identifies the directions (principal components) in which the data varies the most. In PCA, the eigenvectors (principal components) of the covariance matrix provide the directions of maximum variance, and the eigenvalues indicate the magnitude of that variance.</p></li>
<li><p><strong>Efficiency:</strong> Once decomposed, certain operations such as matrix inversion or determining the matrix rank become much easier and more computationally efficient.</p></li>
<li><p><strong>Understanding Data:</strong> By examining the eigenvalues and eigenvectors, one can understand the shape and distribution of the data. Large eigenvalues correspond to dimensions with large variance, indicating that the data spreads out widely in the direction of the corresponding eigenvector.</p></li>
</ol>
<p>Would you like to see a numerical example of how to perform eigenvalue decomposition on a covariance matrix?</p>
</section>
</section>
<section id="how-ols-changed">
<h2>How OLS changed<a class="headerlink" href="#how-ols-changed" title="Permalink to this heading">#</a></h2>
<p>In the context of regression analysis, the ordinary least squares (OLS) estimator for the regression coefficients, often denoted as <span class="math notranslate nohighlight">\( \beta \)</span>, is calculated to minimize the sum of the squared differences between the observed responses and the responses predicted by the linear model. This is applicable in both standard linear regression and Principal Component Regression (PCR), but with a key difference in how the variables are treated.</p>
<section id="ordinary-least-squares-ols">
<h3>Ordinary Least Squares (OLS)<a class="headerlink" href="#ordinary-least-squares-ols" title="Permalink to this heading">#</a></h3>
<p>For standard linear regression, the least squares estimate of <span class="math notranslate nohighlight">\( \beta \)</span> is given by the formula:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{OLS} = (X^\top X)^{-1}X^\top Y
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the matrix of input features,</p></li>
<li><p><span class="math notranslate nohighlight">\( Y \)</span> is the vector of observed responses,</p></li>
<li><p><span class="math notranslate nohighlight">\( X^\top \)</span> is the transpose of <span class="math notranslate nohighlight">\( X \)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\( (X^\top X)^{-1} \)</span> is the inverse of <span class="math notranslate nohighlight">\( X^\top X \)</span> (assuming it is invertible),</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{\beta}_{OLS} \)</span> is the estimated vector of coefficients.</p></li>
</ul>
<p>This estimator minimizes the sum of squared residuals:</p>
<div class="math notranslate nohighlight">
\[
\min_\beta \sum_{i=1}^n (y_i - x_i^\top \beta)^2
\]</div>
</section>
<section id="principal-component-regression-pcr">
<h3>Principal Component Regression (PCR)<a class="headerlink" href="#principal-component-regression-pcr" title="Permalink to this heading">#</a></h3>
<p>In PCR, before you perform regression, you first reduce the dimensionality of <span class="math notranslate nohighlight">\( X \)</span> by selecting a few principal components. Let’s denote the matrix of selected principal components as <span class="math notranslate nohighlight">\( P \)</span>. Then the PCR estimate of <span class="math notranslate nohighlight">\( \beta \)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{PCR} = (P^\top P)^{-1}P^\top Y
\]</div>
<p>Here, the principal component scores matrix <span class="math notranslate nohighlight">\( P \)</span> replaces the original feature matrix <span class="math notranslate nohighlight">\( X \)</span>. After estimating the coefficients <span class="math notranslate nohighlight">\( \hat{\beta}_{PCR} \)</span>, you can transform them back to the original space of <span class="math notranslate nohighlight">\( X \)</span> if needed.</p>
<p>So, while the form of the least squares estimation equation remains similar (it’s always about finding the coefficients that minimize the sum of squared residuals), in PCR, the equation is applied to the principal components of <span class="math notranslate nohighlight">\( X \)</span>, not <span class="math notranslate nohighlight">\( X \)</span> itself. This can lead to a more stable estimate in cases where the original predictors are highly collinear.</p>
</section>
</section>
<section id="limitation-of-pcr">
<h2>Limitation of PCR<a class="headerlink" href="#limitation-of-pcr" title="Permalink to this heading">#</a></h2>
<p>Principal Component Regression (PCR) involves a trade-off between bias and variance, which is a fundamental concept in statistical learning. The limitation of PCR comes from this trade-off, and understanding it requires a look at the underlying concepts:</p>
<section id="bias-variance-trade-off">
<h3>Bias-Variance Trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias</strong> refers to the error introduced by approximating a real-world problem, which may be complex, by a much simpler model. In the context of PCR, using a limited number of principal components means that you are ignoring some information contained in the data, which introduces bias.</p></li>
<li><p><strong>Variance</strong> refers to the error due to the variability of the model prediction if we use different subsets of the data. A model with high variance pays a lot of attention to training data and does not generalize well on unseen data.</p></li>
</ul>
</section>
<section id="limitations-of-pcr">
<h3>Limitations of PCR<a class="headerlink" href="#limitations-of-pcr" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Information Loss</strong>: PCR reduces the dimensionality of the feature space by using a subset of all the principal components, which means it may discard components that contain useful information about the response variable.</p></li>
<li><p><strong>Selection of Components</strong>: Choosing the number of principal components to keep in the model is not always straightforward. Keeping too many components may not sufficiently reduce variance, while keeping too few may introduce significant bias.</p></li>
<li><p><strong>Interpretability</strong>: The principal components are linear combinations of the original variables and may not have a meaningful interpretation. This can make it difficult to understand the model, especially in fields where interpretability is crucial.</p></li>
<li><p><strong>Assumption of Linearity</strong>: PCR assumes that the relationship between the principal components and the response variable is linear. If the true relationship is non-linear, PCR may not provide an adequate model.</p></li>
<li><p><strong>Sensitive to Scaling</strong>: PCR is sensitive to the scaling of the variables. If the variables are on different scales, the principal components may be dominated by the variables with larger scales.</p></li>
</ol>
</section>
</section>
<section id="why-variance-is-traded-off-against-bias">
<h2>Why Variance is Traded Off Against Bias<a class="headerlink" href="#why-variance-is-traded-off-against-bias" title="Permalink to this heading">#</a></h2>
<p>PCR trades variance for bias to create a more robust model that generalizes better to new data. By reducing the dimensionality:</p>
<ul class="simple">
<li><p>It may reduce overfitting: A high-dimensional dataset can lead to a model that fits the noise in the training set rather than the underlying relationship, which is high variance.</p></li>
<li><p>It can improve model performance: By reducing the number of predictors, you often reduce the complexity of the model, which can lead to better performance on new, unseen data, despite introducing some bias.</p></li>
</ul>
<p>In summary, the limitation of PCR is that while it can lead to a model with lower variance, it may introduce bias into the estimation process. The trade-off is beneficial when the reduction in variance leads to improved model performance on unseen data, despite the loss of some information due to bias.</p>
</section>
<section id="difference-between-pcr-and-pls">
<h2>Difference Between PCR and PLS<a class="headerlink" href="#difference-between-pcr-and-pls" title="Permalink to this heading">#</a></h2>
<p>Partial Least Squares (PLS) regression and Principal Component Regression (PCR) are both methods that combine features of principal component analysis (PCA) with regression. They are typically used when the predictor variables are highly collinear or when there are more predictor variables than observations. Despite their similarities, PLS and PCR have distinct differences in terms of methodology and objectives.</p>
<section id="id1">
<h3>Principal Component Regression (PCR):<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Dimensionality Reduction</strong>: PCR is a two-step process where PCA is first performed on the predictor variables to reduce the dimensionality of the problem and to identify a few orthogonal principal components (PCs).</p></li>
<li><p><strong>Regression</strong>: The second step involves performing linear regression on these selected principal components rather than the original predictor variables.</p></li>
<li><p><strong>Variance</strong>: PCR focuses on capturing the variance in the predictor variables without considering the response variable. This might lead to selecting components that explain a lot of the variance in the predictors but have little to do with the response variable.</p></li>
<li><p><strong>Indirect Maximization of Covariance</strong>: Although PCR uses principal components that maximize the variance in the predictors, it does not directly maximize the covariance between the predictors and the response.</p></li>
</ol>
</section>
<section id="partial-least-squares-pls-regression">
<h3>Partial Least Squares (PLS) Regression:<a class="headerlink" href="#partial-least-squares-pls-regression" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Covariance-based Dimensionality Reduction</strong>: PLS, like PCR, is also about reducing the dimensionality of the predictor variables. However, PLS does this by identifying new features (latent variables) that perform a simultaneous decomposition of the predictors and the response variable, with the goal of maximizing the covariance between the response and predictors.</p></li>
<li><p><strong>Integrated Approach</strong>: PLS integrates the dimensionality reduction and the regression steps into a single model, which means that it considers the response variable while identifying the new latent variables.</p></li>
<li><p><strong>Response Variance</strong>: PLS specifically tries to identify components that are relevant for predicting the response variable. This makes PLS more suitable for predictive modeling when the goal is to improve prediction accuracy.</p></li>
<li><p><strong>Direct Maximization of Covariance</strong>: PLS components are chosen to maximize the covariance between the predictors and the response, which often leads to models that are better at prediction.</p></li>
</ol>
</section>
<section id="use-cases">
<h3>Use Cases:<a class="headerlink" href="#use-cases" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>PCR</strong> is often preferred when the primary interest is in understanding the underlying structure of the data or when the components that capture the most variance in the predictors are also expected to be related to the response.</p></li>
<li><p><strong>PLS</strong> is typically used when the goal is predictive modeling, and there’s a need to find the latent structures that most affect the response variable, especially when the predictor variables are numerous and highly collinear.</p></li>
</ul>
<p>In summary, the choice between PCR and PLS depends on the goals of the analysis. If the main interest is in prediction and the predictors are highly collinear, PLS is usually the preferred method. If the goal is dimensionality reduction or if the principal components that capture the most variance are believed to be the most relevant for the response, PCR may be used.</p>
<hr class="docutils" />
<p>Yes, in the context of Partial Least Squares (PLS) regression, the goal is to find components that explain the maximum possible variance in the predictors while also having the highest possible covariance with the response variable. This is distinct from methods like Principal Component Analysis (PCA) and Principal Component Regression (PCR), where the components are chosen solely based on the variance within the predictor variables, without considering the relationship between predictors and the response.</p>
<p>Here’s a bit more detail on how PLS works with respect to maximizing covariance:</p>
<ol class="arabic simple">
<li><p><strong>Projection</strong>: PLS projects both the predictor variables (X) and the response variable (Y) onto a new space. The projections are chosen such that the covariance between the projected X (scores) and projected Y is maximized.</p></li>
<li><p><strong>Covariance</strong>: In statistics, covariance is a measure of the joint variability of two random variables. If you have a set of predictors <span class="math notranslate nohighlight">\( X \)</span> and a response <span class="math notranslate nohighlight">\( Y \)</span>, their covariance is a measure of how much the variables change together. PLS tries to find the linear combinations of the predictor variables that covary the most with the response.</p></li>
<li><p><strong>Latent Variables</strong>: The new components (latent variables) in PLS are constructed as linear combinations of the original predictors. These latent variables are chosen iteratively, where each new latent variable is chosen to maximize the residual covariance between <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> that is not explained by the previous latent variables.</p></li>
<li><p><strong>Regression</strong>: After finding these new components, PLS performs regression on them to predict the response variable. Since these components are designed to have a high covariance with the response, the resulting regression model is often more predictive than one that does not consider the relationship between <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> when reducing dimensionality, like PCR.</p></li>
</ol>
<p>The advantage of maximizing covariance is that PLS tends to find features that are not just explanatory for <span class="math notranslate nohighlight">\( X \)</span> but also predictive of <span class="math notranslate nohighlight">\( Y \)</span>, which makes it particularly useful in situations where prediction is the goal and the predictor variables have multicollinearity. This is in contrast to PCR, where the components are selected only based on how well they capture the variance in <span class="math notranslate nohighlight">\( X \)</span>, which may not always lead to the best predictive performance when it comes to <span class="math notranslate nohighlight">\( Y \)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="review1.0.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">review1</p>
      </div>
    </a>
    <a class="right-next"
       href="review3.0.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">review3</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-cont">PCA Cont</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-decomposition">Eigenvalue decomposition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-matrix">Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decomposition-of-a-covariance-matrix">Decomposition of a Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-decompose-a-covariance-matrix">Why Decompose a Covariance Matrix?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-ols-changed">How OLS changed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-regression-pcr">Principal Component Regression (PCR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitation-of-pcr">Limitation of PCR</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-trade-off">Bias-Variance Trade-off</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-pcr">Limitations of PCR</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-is-traded-off-against-bias">Why Variance is Traded Off Against Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-pcr-and-pls">Difference Between PCR and PLS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Principal Component Regression (PCR):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-least-squares-pls-regression">Partial Least Squares (PLS) Regression:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases:</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>